# -*- coding: utf-8 -*-
"""FRE_LSTM_OrderbookAnalysisipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OXrbynxucOXMPrDQLslHIK--JumnY3ro
"""

import torch
from torch import nn
from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from google.colab import files
uploaded = files.upload()  # Upload `features.bin` and `labels.bin`

!pip install brevitas

# Reload clean data from disk
import numpy as np
import struct

with open("features_v2.bin", "rb") as f:
    num_sequences = struct.unpack("Q", f.read(8))[0]
    vector_size = struct.unpack("Q", f.read(8))[0]
    features = np.frombuffer(f.read(), dtype=np.float64).reshape((num_sequences, vector_size))

with open("labels_v2.bin", "rb") as f:
    num_labels = struct.unpack("Q", f.read(8))[0]
    labels = np.frombuffer(f.read(), dtype=np.int32)

print("Feature shape:", features.shape)
print("Label distribution:", np.bincount(labels))

# Summary info (you already have this)
print("Feature shape:", features.shape)
print("Labels shape:", labels.shape)
print("Label distribution:", np.bincount(labels))
print("Any NaNs?", np.isnan(features).any())
print("Any Infs?", np.isinf(features).any())
print("Max value:", np.max(features))
print("Min value:", np.min(features))

# Peek into a few feature vectors by class
num_samples_to_print = 3

for label in [0, 1, 2]:  # 0=Up, 1=Down, 2=No Change
    print(f"\n--- Sample features for class {label} ---")
    count = 0
    for i in range(len(labels)):
        if labels[i] == label:
            print(f"Sample #{i} (label={label}):")
            print(features[i][:10], "...")  # print first 10 features only
            count += 1
            if count >= num_samples_to_print:
                break

# Replace inf with large finite numbers
features = np.where(np.isinf(features), np.sign(features) * 1e6, features)

# Replace NaN with 0 (or mean if preferred)
features = np.nan_to_num(features, nan=0.0)

scaler = StandardScaler()
features = scaler.fit_transform(features)

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    features, labels, test_size=0.2, random_state=42, stratify=labels
)

# Convert to PyTorch tensors
X_train_t = torch.tensor(X_train, dtype=torch.float32)
y_train_t = torch.tensor(y_train, dtype=torch.long)
X_test_t = torch.tensor(X_test, dtype=torch.float32)
y_test_t = torch.tensor(y_test, dtype=torch.long)

sequence_length = 10  # if you used that
feature_dim = X_train.shape[1] // sequence_length

# Reshape to (batch, seq, features)
X_train_t = X_train_t.view(-1, sequence_length, feature_dim)
X_test_t = X_test_t.view(-1, sequence_length, feature_dim)

train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=64, shuffle=True)
test_loader = DataLoader(TensorDataset(X_test_t, y_test_t), batch_size=64)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

from brevitas.nn import QuantLSTM, QuantLinear
from brevitas.quant import Int8ActPerTensorFixedPoint, Int8WeightPerTensorFixedPoint

class QuantLSTMModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(QuantLSTMModel, self).__init__()
        self.hidden_size = hidden_size
        self.lstm = QuantLSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            batch_first=True,
            weight_quant=Int8WeightPerTensorFixedPoint,
            io_quant=Int8ActPerTensorFixedPoint
        )
        self.fc = QuantLinear(
            in_features=hidden_size,
            out_features=num_classes,
            bias=True,
            weight_quant=Int8WeightPerTensorFixedPoint,
            act_quant=Int8ActPerTensorFixedPoint
        )

    def forward(self, x):
        out, _ = self.lstm(x)  # Brevitas LSTM handles hidden state internally
        out = self.fc(out[:, -1, :])  # Grab output at last timestep
        return out

model = QuantLSTMModel(input_size=feature_dim, hidden_size=64, num_classes=3).to(device)

# Define class weights manually based on inverse class frequency
# [Up, Down, No Change] = [2425, 10, 57839]
class_counts = torch.tensor([884, 412, 59423], dtype=torch.float32)
inv_freqs = 1.0 / class_counts
weights = inv_freqs / inv_freqs.sum()  # Normalize
weights = weights.to(device)

criterion = nn.CrossEntropyLoss(weight=weights)

optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

from sklearn.utils import resample

X_up = features[labels == 0]
y_up = labels[labels == 0]

X_down = features[labels == 1]
y_down = labels[labels == 1]

X_nc = features[labels == 2]
y_nc = labels[labels == 2]

X_up_os, y_up_os = resample(X_up, y_up, replace=True, n_samples=3000)
X_down_os, y_down_os = resample(X_down, y_down, replace=True, n_samples=3000)
X_nc_ds, y_nc_ds = resample(X_nc, y_nc, replace=False, n_samples=3000)

# Combine and shuffle
X_balanced = np.vstack([X_up_os, X_down_os, X_nc_ds])
y_balanced = np.concatenate([y_up_os, y_down_os, y_nc_ds])

from sklearn.metrics import f1_score

best_f1 = 0
num_epochs = 15  # or higher

for epoch in range(num_epochs):
    model.train()
    total_loss = 0

    for xb, yb in train_loader:
        xb, yb = xb.to(device), yb.to(device)
        optimizer.zero_grad()
        outputs = model(xb)
        loss = criterion(outputs, yb)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    print(f"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}")

    # Eval
    model.eval()
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for xb, yb in test_loader:
            xb, yb = xb.to(device), yb.to(device)
            outputs = model(xb)

            all_preds.append(outputs.cpu())     # Append raw logits
            all_labels.append(yb.cpu())         # Append true labels

    all_preds = torch.cat(all_preds).argmax(dim=1).numpy()
    all_labels = torch.cat(all_labels).numpy()
    f1 = f1_score(all_labels, all_preds, average="macro")
    print(f"Epoch {epoch+1}, F1: {f1:.4f}")

    if f1 > best_f1:
        best_f1 = f1
        torch.save(model.state_dict(), "best_model.pth")

model.eval()
all_preds, all_labels = [], []
with torch.no_grad():
    for xb, yb in test_loader:
        xb = xb.to(device)
        outputs = model(xb)
        preds = torch.argmax(outputs, dim=1).cpu().numpy()
        all_preds.extend(preds)
        all_labels.extend(yb.numpy())

from sklearn.metrics import classification_report
print(classification_report(all_labels, all_preds, target_names=["Up", "Down", "No Change"], zero_division=0))

from sklearn.metrics import classification_report, confusion_matrix

y_true = []
y_pred = []

model.eval()
with torch.no_grad():
    for x_batch, y_batch in test_loader:
        out = model(x_batch)
        preds = torch.argmax(out, dim=1)
        y_true.extend(y_batch.cpu().numpy())
        y_pred.extend(preds.cpu().numpy())

print(confusion_matrix(y_true, y_pred))
print(classification_report(y_true, y_pred, target_names=["Up", "Down", "No Change"]))

import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_true, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=["Up", "Down", "No Change"],
            yticklabels=["Up", "Down", "No Change"])
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

from sklearn.metrics import precision_recall_fscore_support

prec, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, labels=[0, 1, 2])
print(f"Up     -> P: {prec[0]:.2f}, R: {recall[0]:.2f}, F1: {f1[0]:.2f}")
print(f"Down   -> P: {prec[1]:.2f}, R: {recall[1]:.2f}, F1: {f1[1]:.2f}")
print(f"No Chg -> P: {prec[2]:.2f}, R: {recall[2]:.2f}, F1: {f1[2]:.2f}")

from sklearn.manifold import TSNE

embeddings = []
labels = []

model.eval()
with torch.no_grad():
    for xb, yb in test_loader:
        xb = xb.to(device)
        out, _ = model.lstm(xb)
        last_step = out[:, -1, :].cpu()
        embeddings.append(last_step)
        labels.append(yb)

embeddings = torch.cat(embeddings).numpy()
labels = torch.cat(labels).numpy()

tsne = TSNE(n_components=2, perplexity=30)
X_tsne = tsne.fit_transform(embeddings)

plt.figure(figsize=(8, 6))
plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap="viridis", alpha=0.5)
plt.title("LSTM Latent Space (t-SNE)")
plt.colorbar(label="Class")
plt.show()

!pip install onnx

model.eval()
dummy_input = torch.randn(1, sequence_length, feature_dim).to(device)
with torch.no_grad():
    torch.onnx.export(
        model,
        dummy_input,
        "quant_lstm.onnx",
        export_params=True,
        opset_version=11,
        input_names=["input"],
        output_names=["output"]
    )

from google.colab import files
files.download("quant_lstm.onnx")

